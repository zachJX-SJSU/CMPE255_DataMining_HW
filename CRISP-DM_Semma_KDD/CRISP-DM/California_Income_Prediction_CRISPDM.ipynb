{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98234819",
   "metadata": {},
   "source": [
    "\n",
    "# California Housing — Predicting Community Median Income (CRISP‑DM, scikit‑learn)\n",
    "\n",
    "This notebook is a **textbook‑quality CRISP‑DM walkthrough** using the Kaggle dataset  \n",
    "**“California Housing Prices”** (by *camnugent*). We **predict `median_income`** from the\n",
    "remaining variables (location, housing stock, demographics). The pipeline is **compute‑aware**\n",
    "and designed for **Google Colab**.\n",
    "\n",
    "**Phases covered**\n",
    "- Business Understanding → Data Understanding → Data Preparation (cleaning, preprocessing)\n",
    "- Feature Engineering & Selection → Outlier analysis/processing → Clustering (unsupervised segmentation)\n",
    "- Baselines & Model Benchmarking (geo‑aware CV) → Final Holdout Evaluation & Explainability\n",
    "- Final Recommendation, Deployment/Monitoring notes\n",
    "\n",
    "> Units: `median_income` is approximately **$10,000s of 1990 USD**. We report MAE in model units and dollars.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81981243",
   "metadata": {},
   "source": [
    "finished"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fe6c4f",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Environment Setup (Colab)\n",
    "- Installs: `scikit-learn`, `shap` (optional), and `kaggle` (for dataset download).\n",
    "- Sets a fast/compute‑aware configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcea78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running on Colab, you can keep these. On local, adjust as needed.\n",
    "!pip -q install -U scikit-learn shap kaggle\n",
    "\n",
    "FAST = True                 # Set to False for more exhaustive tuning\n",
    "RANDOM_SEED = 42\n",
    "N_JOBS = -1\n",
    "CV_SPLITS = 5\n",
    "\n",
    "# Compact, compute-aware defaults\n",
    "LINEAR_N_ITER = 15 if FAST else 25\n",
    "RF_N_ITER     = 16 if FAST else 32\n",
    "HGB_N_ITER    = 20 if FAST else 40\n",
    "BOOTSTRAP_B   = 1000 if FAST else 2000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b16152",
   "metadata": {},
   "source": [
    "finished"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c3e87e",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Data Access (Kaggle or Manual Upload)\n",
    "Preferred: **Kaggle API**. Alternative: manual upload of `housing.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063b1050",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, zipfile, pathlib, json\n",
    "\n",
    "DATA_DIR = \"/content/data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "CSV_PATH = os.path.join(DATA_DIR, \"housing.csv\")\n",
    "\n",
    "if not os.path.exists(CSV_PATH):\n",
    "    kaggle_creds = os.path.expanduser(\"~/.kaggle/kaggle.json\")\n",
    "    if os.path.exists(kaggle_creds):\n",
    "        print(\"Kaggle credentials found. Downloading dataset...\")\n",
    "        # Requires Kaggle API to be configured (kaggle.json placed at ~/.kaggle with correct permissions).\n",
    "        # Dataset: camnugent/california-housing-prices\n",
    "        !kaggle datasets download -d camnugent/california-housing-prices -p $DATA_DIR --unzip\n",
    "    else:\n",
    "        print(\"Kaggle credentials not found. Please either:\")\n",
    "        print(\"  1) Upload ~/.kaggle/kaggle.json and re-run this cell, or\")\n",
    "        print(\"  2) Use the next cell to upload housing.csv manually.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bac0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Manual upload (Colab). Run this cell if the CSV is not already present.\n",
    "# After upload, if the file isn't named 'housing.csv', we place/rename it accordingly.\n",
    "import os\n",
    "if not os.path.exists(CSV_PATH):\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        uploaded = files.upload()\n",
    "        for fn in uploaded.keys():\n",
    "            if fn.lower().endswith(\".csv\"):\n",
    "                os.replace(fn, CSV_PATH)\n",
    "                print(\"Saved uploaded CSV to\", CSV_PATH)\n",
    "    except Exception as e:\n",
    "        print(\"Manual upload not available in this environment:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf90dec",
   "metadata": {},
   "source": [
    "finished"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6c93ce",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Imports & Global Helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c7555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings, math\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from math import sqrt\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(transform_output=\"default\")\n",
    "\n",
    "RNG = np.random.RandomState(RANDOM_SEED)\n",
    "\n",
    "def to_usd(x): return x * 10_000  # convert model units (~$10k) to dollars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c06bba",
   "metadata": {},
   "source": [
    "finished"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de25830d",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Data Understanding — Load & Initial Audit\n",
    "- Confirm shape, dtypes, missingness, duplicates.\n",
    "- Sanity checks on coordinates and logical constraints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa9ef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_raw = pd.read_csv(CSV_PATH)\n",
    "df = df_raw.copy()\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "print(df.dtypes)\n",
    "print(f\"Memory MB: {df.memory_usage(deep=True).sum()/1_048_576:.2f}\")\n",
    "display(df.head(3))\n",
    "display(df.describe())\n",
    "display(df.describe(include='object'))\n",
    "print(\"Missing counts:\\n\", df.isna().sum().sort_values(ascending=False))\n",
    "print(\"Duplicates:\", df.duplicated().sum())\n",
    "\n",
    "# Quick target profile\n",
    "t = df['median_income'].agg(['min','max','mean','median','std'])\n",
    "print(\"Target stats (median_income; ~$10k units):\\n\", t)\n",
    "print(\"Skew:\", df['median_income'].skew())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27e1eb1",
   "metadata": {},
   "source": [
    "finished"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9aa067",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Cleaning (minimal, auditable)\n",
    "- Drop out-of-CA coordinates and impossible/illogical rows.\n",
    "- Keep missing `total_bedrooms` to impute later.\n",
    "- Cast `ocean_proximity` to category; drop duplicates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a69d634",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_raw(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # Bounds for California\n",
    "    lat_ok = df['latitude'].between(32, 42.5)\n",
    "    lon_ok = df['longitude'].between(-125, -113)\n",
    "    df = df[lat_ok & lon_ok]\n",
    "    # Logical constraints\n",
    "    df = df[df['households'] > 0]\n",
    "    df = df[df['total_rooms'] > 0]\n",
    "    df = df[df['total_bedrooms'].isna() | (df['total_bedrooms'] >= 0)]\n",
    "    df = df[df['population'] >= df['households']]\n",
    "    df = df[(df['total_bedrooms'].isna()) | (df['total_rooms'] >= df['total_bedrooms'])]\n",
    "    # Types & duplicates\n",
    "    if df['ocean_proximity'].dtype.name != 'category':\n",
    "        df['ocean_proximity'] = df['ocean_proximity'].astype('category')\n",
    "    df = df.drop_duplicates()\n",
    "    return df\n",
    "\n",
    "df = clean_raw(df)\n",
    "print(\"After cleaning:\", df.shape)\n",
    "print(df.isna().sum().sort_values(ascending=False).head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3949aa7",
   "metadata": {},
   "source": [
    "finished"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629b2e88",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Feature Engineering & Robust Transforms\n",
    "- Engineered ratios: rooms/household, bedrooms/room, population/household.\n",
    "- Winsorization for heavy tails; log1p for large counts.\n",
    "- Light spatial polynomials (`lat^2`, `lon^2`, `lat*lon`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de57710",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "TARGET = 'median_income'\n",
    "CORE_BASE = ['longitude','latitude','housing_median_age',\n",
    "             'total_rooms','total_bedrooms','population','households','ocean_proximity']\n",
    "\n",
    "class AddRatios(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, eps=1e-6): self.eps = eps\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X['rooms_per_household']      = X['total_rooms']    / (X['households'] + self.eps)\n",
    "        X['bedrooms_per_room']        = X['total_bedrooms'] / (X['total_rooms'] + self.eps)\n",
    "        X['population_per_household'] = X['population']     / (X['households'] + self.eps)\n",
    "        X['lat2']   = X['latitude']**2\n",
    "        X['lon2']   = X['longitude']**2\n",
    "        X['lat_lon']= X['latitude']*X['longitude']\n",
    "        return X\n",
    "\n",
    "RATIO_COLS = ['rooms_per_household','bedrooms_per_room','population_per_household','lat2','lon2','lat_lon']\n",
    "NUM_BASE   = ['longitude','latitude','housing_median_age','total_rooms','total_bedrooms','population','households']\n",
    "CAT_COLS   = ['ocean_proximity']\n",
    "NUM_ALL    = NUM_BASE + RATIO_COLS\n",
    "\n",
    "class Winsorize(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols, lower=0.005, upper=0.995):\n",
    "        self.cols, self.lower, self.upper = cols, lower, upper\n",
    "    def fit(self, X, y=None):\n",
    "        X = X if isinstance(X, pd.DataFrame) else pd.DataFrame(X, columns=self.cols)\n",
    "        self.bounds_ = {c: (X[c].quantile(self.lower), X[c].quantile(self.upper)) for c in self.cols if c in X}\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for c,(lo,hi) in self.bounds_.items():\n",
    "            if c in X: X[c] = X[c].clip(lo, hi)\n",
    "        if 'bedrooms_per_room' in X:\n",
    "            X['bedrooms_per_room'] = X['bedrooms_per_room'].clip(0, 1)\n",
    "        return X\n",
    "\n",
    "class Log1pCols(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols): self.cols = cols\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for c in self.cols:\n",
    "            if c in X: X[c] = np.log1p(np.maximum(X[c], 0))\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effecd91",
   "metadata": {},
   "source": [
    "finished"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f4bb9f",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Preprocessing Pipelines (Linear vs Tree)\n",
    "- Linear: median impute + standardize; Tree: median impute only.\n",
    "- Categorical: `OneHotEncoder(handle_unknown='ignore')` (dense).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d583a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def make_ohe():\n",
    "    # Backward-compatible OHE for older scikit-learn\n",
    "    try:\n",
    "        return OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "cat_common = Pipeline([('impute', SimpleImputer(strategy='most_frequent')),\n",
    "                       ('onehot', make_ohe())])\n",
    "\n",
    "num_linear = Pipeline([('impute', SimpleImputer(strategy='median')),\n",
    "                       ('scale',  StandardScaler())])\n",
    "\n",
    "num_tree   = SimpleImputer(strategy='median')\n",
    "\n",
    "preproc_linear = ColumnTransformer([\n",
    "    ('num', num_linear, NUM_ALL),\n",
    "    ('cat', cat_common, CAT_COLS)\n",
    "], remainder='drop')\n",
    "\n",
    "preproc_tree = ColumnTransformer([\n",
    "    ('num', num_tree, NUM_ALL),\n",
    "    ('cat', cat_common, CAT_COLS)\n",
    "], remainder='drop')\n",
    "\n",
    "# Full feature pipelines\n",
    "clip_cols = NUM_BASE + RATIO_COLS\n",
    "log_cols  = ['total_rooms','total_bedrooms','population','households']\n",
    "\n",
    "pipe_linear_features = Pipeline([\n",
    "    ('add',    AddRatios()),\n",
    "    ('winsor', Winsorize(cols=clip_cols, lower=0.005, upper=0.995)),\n",
    "    ('log1p',  Log1pCols(cols=log_cols)),\n",
    "    ('prep',   preproc_linear)\n",
    "])\n",
    "\n",
    "pipe_tree_features = Pipeline([\n",
    "    ('add',    AddRatios()),\n",
    "    ('winsor', Winsorize(cols=clip_cols, lower=0.005, upper=0.995)),\n",
    "    ('prep',   preproc_tree)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc591b4",
   "metadata": {},
   "source": [
    "finished"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d027d610",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Geographic Split (Leakage‑aware)\n",
    "We create 0.5° lat‑lon tiles and use them as **groups** for GroupKFold CV and\n",
    "to form a **geographically disjoint** test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50f9dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "def make_geo_groups(df, tile_deg=0.5):\n",
    "    lat_bin = np.floor(df['latitude']  / tile_deg).astype(int)\n",
    "    lon_bin = np.floor(df['longitude'] / tile_deg).astype(int)\n",
    "    return (lat_bin.astype(str) + \"_\" + lon_bin.astype(str)).astype('category')\n",
    "\n",
    "groups = make_geo_groups(df, tile_deg=0.5)\n",
    "unique_groups = pd.Series(groups).unique()\n",
    "\n",
    "RNG = np.random.RandomState(RANDOM_SEED)\n",
    "test_groups = RNG.choice(unique_groups, size=int(np.ceil(0.20*len(unique_groups))), replace=False)\n",
    "is_test = groups.isin(test_groups)\n",
    "\n",
    "X_core = df[['longitude','latitude','housing_median_age','total_rooms','total_bedrooms',\n",
    "             'population','households','ocean_proximity']].copy()\n",
    "y = df[TARGET].copy()\n",
    "\n",
    "X_train, X_test = X_core[~is_test], X_core[is_test]\n",
    "y_train, y_test = y[~is_test], y[is_test]\n",
    "g_train, g_test = groups[~is_test], groups[is_test]\n",
    "\n",
    "gkf = GroupKFold(n_splits=CV_SPLITS)\n",
    "print(\"Train/Test sizes:\", X_train.shape, X_test.shape)\n",
    "print(\"Unique test tiles:\", len(pd.unique(g_test)))\n",
    "assert set(pd.unique(g_train)).isdisjoint(set(pd.unique(g_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90de3a04",
   "metadata": {},
   "source": [
    "finished"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077378fb",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Outlier Analysis & Mutual Information (quick)\n",
    "We flag univariate outliers (IQR/MAD) and compute **mutual information** with the target\n",
    "on preprocessed features to inform selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63732fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import median_abs_deviation\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "def mad_zscore(s):\n",
    "    med = np.nanmedian(s); mad = median_abs_deviation(s, nan_policy='omit', scale='normal')\n",
    "    return (s - med) / (mad if mad>0 else 1.0)\n",
    "\n",
    "NUM_SIMPLE = ['longitude','latitude','housing_median_age','total_rooms','total_bedrooms','population','households']\n",
    "flags = {}\n",
    "for c in NUM_SIMPLE:\n",
    "    s = df[c]\n",
    "    iqr = s.quantile(0.75) - s.quantile(0.25)\n",
    "    iqr_mask = (s < s.quantile(0.25) - 1.5*iqr) | (s > s.quantile(0.75) + 1.5*iqr)\n",
    "    mz_mask  = mad_zscore(s).abs() > 3.5\n",
    "    flags[c] = dict(iqr=float(iqr_mask.mean()), mad=float(mz_mask.mean()))\n",
    "pd.DataFrame(flags).T.sort_values('mad', ascending=False)\n",
    "\n",
    "# Mutual information on train\n",
    "Xt = pipe_linear_features.fit_transform(X_train, y_train)\n",
    "feat_names = pipe_linear_features.named_steps['prep'].get_feature_names_out()\n",
    "mi = mutual_info_regression(Xt, y_train, random_state=RANDOM_SEED)\n",
    "mi_tbl = pd.DataFrame({'feature': feat_names, 'MI': mi}).sort_values('MI', ascending=False)\n",
    "mi_tbl.head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3405f198",
   "metadata": {},
   "source": [
    "finished"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5551d1b",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Clustering (structure discovery)\n",
    "We use **MiniBatchKMeans** on scaled engineered features to derive an unsupervised\n",
    "`cluster_id` for segmentation and potential modeling gains.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03538fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "CLUST_COLS = ['latitude','longitude','housing_median_age',\n",
    "              'rooms_per_household','bedrooms_per_room','population_per_household']\n",
    "\n",
    "# Prepare train-only engineered features for clustering\n",
    "prep_for_cluster = Pipeline([\n",
    "    ('add',    AddRatios()),\n",
    "    ('winsor', Winsorize(cols=NUM_BASE + RATIO_COLS, lower=0.005, upper=0.995))\n",
    "])\n",
    "\n",
    "Xtr_eng = prep_for_cluster.fit_transform(X_train.copy())\n",
    "Xs = StandardScaler().fit_transform(Xtr_eng[CLUST_COLS])\n",
    "\n",
    "# Pick k via silhouette on a sample\n",
    "idx = RNG.choice(len(Xs), size=min(5000, len(Xs)), replace=False)\n",
    "results = []\n",
    "for k in range(3, 11):\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=1024, random_state=RANDOM_SEED, n_init=10)\n",
    "    lbl = km.fit_predict(Xs)\n",
    "    sil = silhouette_score(Xs[idx], lbl[idx])\n",
    "    inertia = km.inertia_\n",
    "    results.append((k, sil, inertia))\n",
    "\n",
    "sil_tbl = pd.DataFrame(results, columns=['k','silhouette','inertia']).sort_values('silhouette', ascending=False)\n",
    "display(sil_tbl)\n",
    "\n",
    "BEST_K = int(sil_tbl.iloc[0]['k']) if len(sil_tbl) else 6\n",
    "print(\"Chosen k:\", BEST_K)\n",
    "\n",
    "# Fit final kmeans on train\n",
    "scaler = StandardScaler().fit(Xtr_eng[CLUST_COLS])\n",
    "kmeans = MiniBatchKMeans(n_clusters=BEST_K, batch_size=1024, random_state=RANDOM_SEED, n_init=10).fit(scaler.transform(Xtr_eng[CLUST_COLS]))\n",
    "\n",
    "# Assign labels\n",
    "X_train_cl = X_train.copy()\n",
    "X_train_cl['cluster_id'] = pd.Categorical(kmeans.predict(scaler.transform(Xtr_eng[CLUST_COLS])))\n",
    "\n",
    "Xte_eng = prep_for_cluster.transform(X_test.copy())\n",
    "X_test_cl = X_test.copy()\n",
    "X_test_cl['cluster_id'] = pd.Categorical(kmeans.predict(scaler.transform(Xte_eng[CLUST_COLS])))\n",
    "\n",
    "# Inspect clusters (centroids approx inverse-scaled)\n",
    "centroids = pd.DataFrame(scaler.inverse_transform(kmeans.cluster_centers_), columns=CLUST_COLS)\n",
    "centroids['cluster_id'] = range(BEST_K)\n",
    "display(centroids.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39c605e",
   "metadata": {},
   "source": [
    "finished"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327aa698",
   "metadata": {},
   "source": [
    "\n",
    "## 11) Cluster‑aware Preprocessing (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98e692e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class KMeansClusterer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols, n_clusters=6, random_state=42):\n",
    "        self.cols, self.n_clusters, self.random_state = cols, n_clusters, random_state\n",
    "    def fit(self, X, y=None):\n",
    "        Xp = prep_for_cluster.fit_transform(X)\n",
    "        self.scaler_ = StandardScaler().fit(Xp[self.cols])\n",
    "        self.km_ = MiniBatchKMeans(n_clusters=self.n_clusters, batch_size=1024,\n",
    "                                   random_state=self.random_state, n_init=10).fit(self.scaler_.transform(Xp[self.cols]))\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        Xp = prep_for_cluster.transform(X)\n",
    "        lbl = self.km_.predict(self.scaler_.transform(Xp[self.cols]))\n",
    "        X['cluster_id'] = pd.Categorical(lbl)\n",
    "        return X\n",
    "\n",
    "CAT_WITH_CLUSTER = ['ocean_proximity','cluster_id']\n",
    "\n",
    "cat_common2 = Pipeline([('impute', SimpleImputer(strategy='most_frequent')),\n",
    "                        ('onehot', make_ohe())])\n",
    "\n",
    "preproc_linear_with_cluster = ColumnTransformer([\n",
    "    ('num', Pipeline([('impute', SimpleImputer(strategy='median')), ('scale', StandardScaler())]), NUM_ALL),\n",
    "    ('cat', cat_common2, CAT_WITH_CLUSTER)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d796554",
   "metadata": {},
   "source": [
    "finished"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7101c782",
   "metadata": {},
   "source": [
    "\n",
    "## 12) Baselines & Model Benchmarking (geo‑aware CV)\n",
    "Primary: **MAE**. Secondary: **RMSE**, **R²**. Uses **GroupKFold** over geographic tiles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e059f9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import cross_validate, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from scipy.stats import loguniform, uniform\n",
    "\n",
    "cv_splits = list(gkf.split(X_train, y_train, groups=g_train))\n",
    "SCORING = {'mae': 'neg_mean_absolute_error',\n",
    "           'mse': 'neg_mean_squared_error',\n",
    "           'r2':  'r2'}\n",
    "\n",
    "def summarize_cv(cv_out, name):\n",
    "    mae  = -cv_out['test_mae']\n",
    "    rmse = np.sqrt(-cv_out['test_mse'])\n",
    "    return dict(model=name,\n",
    "                MAE_mean=mae.mean(), MAE_std=mae.std(),\n",
    "                RMSE_mean=rmse.mean(), RMSE_std=rmse.std(),\n",
    "                R2_mean=cv_out['test_r2'].mean(), R2_std=cv_out['test_r2'].std())\n",
    "\n",
    "results = []\n",
    "\n",
    "# Dummy baseline\n",
    "m_dummy = DummyRegressor(strategy='mean')\n",
    "cv = cross_validate(m_dummy, X_train, y_train, cv=cv_splits, scoring=SCORING)\n",
    "results.append(summarize_cv(cv, \"Dummy\"))\n",
    "\n",
    "# Linear baseline\n",
    "m_lr = Pipeline([('X', pipe_linear_features), ('lr', LinearRegression())])\n",
    "cv = cross_validate(m_lr, X_train, y_train, cv=cv_splits, scoring=SCORING)\n",
    "results.append(summarize_cv(cv, \"Linear (no cluster)\"))\n",
    "\n",
    "# Cluster-augmented linear baseline\n",
    "m_lr_cl = Pipeline([\n",
    "    ('cluster', KMeansClusterer(cols=CLUST_COLS, n_clusters=BEST_K, random_state=RANDOM_SEED)),\n",
    "    ('add',     AddRatios()),\n",
    "    ('winsor',  Winsorize(cols=NUM_BASE + RATIO_COLS, lower=0.005, upper=0.995)),\n",
    "    ('log1p',   Log1pCols(cols=['total_rooms','total_bedrooms','population','households'])),\n",
    "    ('prep',    preproc_linear_with_cluster),\n",
    "    ('lr',      LinearRegression())\n",
    "])\n",
    "cv = cross_validate(m_lr_cl, X_train, y_train, cv=cv_splits, scoring=SCORING)\n",
    "results.append(summarize_cv(cv, \"Linear +Cluster\"))\n",
    "\n",
    "# Helper to tune linear family\n",
    "def tune_linear(base_estimator, param_grid, name, n_iter=LINEAR_N_ITER):\n",
    "    pipe = Pipeline([('X', pipe_linear_features), (name, base_estimator)])\n",
    "    search = RandomizedSearchCV(\n",
    "        pipe, param_distributions=param_grid, n_iter=n_iter,\n",
    "        cv=cv_splits, scoring='neg_mean_absolute_error',\n",
    "        n_jobs=N_JOBS, random_state=RANDOM_SEED, refit=True\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "    cv = cross_validate(search.best_estimator_, X_train, y_train, cv=cv_splits, scoring=SCORING)\n",
    "    res = summarize_cv(cv, f\"{name} (best)\")\n",
    "    res['best_params'] = search.best_params_\n",
    "    return search, res\n",
    "\n",
    "# Regularized linear models\n",
    "ridge_search, res_ridge = tune_linear(Ridge(), { 'Ridge__alpha': loguniform(1e-3, 1e3) }, 'Ridge')\n",
    "lasso_search, res_lasso = tune_linear(Lasso(max_iter=10000), { 'Lasso__alpha': loguniform(1e-4, 1e1) }, 'Lasso')\n",
    "enet_search,  res_enet  = tune_linear(ElasticNet(max_iter=10000),\n",
    "                                      { 'ElasticNet__alpha': loguniform(1e-4, 1e1),\n",
    "                                        'ElasticNet__l1_ratio': uniform(0.05, 0.9) }, 'ElasticNet')\n",
    "\n",
    "results += [res_ridge, res_lasso, res_enet]\n",
    "\n",
    "# Random Forest\n",
    "rf_pipe = Pipeline([('X', pipe_tree_features),\n",
    "                    ('rf', RandomForestRegressor(random_state=RANDOM_SEED, n_jobs=N_JOBS))])\n",
    "\n",
    "rf_grid = {\n",
    "    'rf__n_estimators': [200, 300, 400, 600],\n",
    "    'rf__max_depth': [None, 10, 15, 20],\n",
    "    'rf__min_samples_leaf': [1, 2, 4, 8],\n",
    "    'rf__max_features': [0.5, 0.7, 0.9, 1.0]\n",
    "}\n",
    "rf_search = RandomizedSearchCV(rf_pipe, rf_grid, n_iter=RF_N_ITER, cv=cv_splits,\n",
    "                               scoring='neg_mean_absolute_error',\n",
    "                               n_jobs=N_JOBS, random_state=RANDOM_SEED, refit=True)\n",
    "rf_search.fit(X_train, y_train)\n",
    "cv = cross_validate(rf_search.best_estimator_, X_train, y_train, cv=cv_splits, scoring=SCORING)\n",
    "res_rf = summarize_cv(cv, \"RandomForest (best)\")\n",
    "res_rf['best_params'] = rf_search.best_params_\n",
    "results.append(res_rf)\n",
    "\n",
    "# HistGradientBoosting\n",
    "hgb_pipe = Pipeline([('X', pipe_tree_features),\n",
    "                     ('hgb', HistGradientBoostingRegressor(random_state=RANDOM_SEED,\n",
    "                                                           early_stopping=True, validation_fraction=0.1))])\n",
    "\n",
    "from scipy.stats import loguniform as logu\n",
    "hgb_grid = {\n",
    "    'hgb__learning_rate': logu(0.03, 0.5),\n",
    "    'hgb__max_depth': [None, 6, 8, 12],\n",
    "    'hgb__min_samples_leaf': [10, 20, 30, 50, 60],\n",
    "    'hgb__l2_regularization': logu(1e-10, 1e-2),\n",
    "    'hgb__max_bins': [128, 255]\n",
    "}\n",
    "hgb_search = RandomizedSearchCV(hgb_pipe, hgb_grid, n_iter=HGB_N_ITER, cv=cv_splits,\n",
    "                                scoring='neg_mean_absolute_error',\n",
    "                                n_jobs=N_JOBS, random_state=RANDOM_SEED, refit=True)\n",
    "hgb_search.fit(X_train, y_train)\n",
    "cv = cross_validate(hgb_search.best_estimator_, X_train, y_train, cv=cv_splits, scoring=SCORING)\n",
    "res_hgb = summarize_cv(cv, \"HistGB (best)\")\n",
    "res_hgb['best_params'] = hgb_search.best_params_\n",
    "results.append(res_hgb)\n",
    "\n",
    "res_df = pd.DataFrame(results).sort_values('MAE_mean')\n",
    "display(res_df[['model','MAE_mean','MAE_std','RMSE_mean','R2_mean']])\n",
    "\n",
    "candidates = {\n",
    "    'Ridge': ridge_search.best_estimator_,\n",
    "    'Lasso': lasso_search.best_estimator_,\n",
    "    'ElasticNet': enet_search.best_estimator_,\n",
    "    'RandomForest': rf_search.best_estimator_,\n",
    "    'HistGB': hgb_search.best_estimator_,\n",
    "    'Linear (no cluster)': m_lr,\n",
    "    'Linear +Cluster': m_lr_cl\n",
    "}\n",
    "champion_name = res_df.iloc[0]['model']\n",
    "# Normalize key name before \" (best)\"\n",
    "champ_key = champion_name.split(' (')[0]\n",
    "champion = candidates.get(champ_key, hgb_search.best_estimator_)\n",
    "print(\"Champion by CV MAE:\", champion_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deff4668",
   "metadata": {},
   "source": [
    "finished"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4460d0b6",
   "metadata": {},
   "source": [
    "\n",
    "## 13) Final Training on Train Tiles & Holdout Evaluation on Disjoint Test Tiles\n",
    "We compare the champion to **Dummy** and a **Linear baseline**.\n",
    "We also produce 95% **bootstrap CIs** for MAE/RMSE/R².\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24650bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "# Refit\n",
    "champion.fit(X_train, y_train)\n",
    "dummy = DummyRegressor(strategy='mean').fit(X_train, y_train)\n",
    "linear_baseline = Pipeline([('X', pipe_linear_features), ('lr', LinearRegression())]).fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = champion.predict(X_test)\n",
    "y_dummy = dummy.predict(X_test)\n",
    "y_lin   = linear_baseline.predict(X_test)\n",
    "\n",
    "def reg_metrics(y_true, y_hat):\n",
    "    mae  = mean_absolute_error(y_true, y_hat)\n",
    "    rmse = sqrt(mean_squared_error(y_true, y_hat))\n",
    "    r2   = r2_score(y_true, y_hat)\n",
    "    return {'MAE': mae, 'RMSE': rmse, 'R2': r2}\n",
    "\n",
    "test_metrics = {'Champion': reg_metrics(y_test, y_pred),\n",
    "                'Dummy':    reg_metrics(y_test, y_dummy),\n",
    "                'Linear':   reg_metrics(y_test, y_lin)}\n",
    "pd.DataFrame(test_metrics).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f530c5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Bootstrap CIs (compute-aware)\n",
    "RNG = np.random.RandomState(RANDOM_SEED)\n",
    "\n",
    "def bootstrap_ci(y_true, y_hat, fn, B=BOOTSTRAP_B, alpha=0.05):\n",
    "    n = len(y_true); stats = np.empty(B)\n",
    "    for b in range(B):\n",
    "        idx = RNG.randint(0, n, n)\n",
    "        stats[b] = fn(y_true[idx], y_hat[idx])\n",
    "    lo, hi = np.quantile(stats, [alpha/2, 1-alpha/2])\n",
    "    return stats.mean(), (lo, hi)\n",
    "\n",
    "mae_mean, (mae_lo, mae_hi) = bootstrap_ci(y_test.values, y_pred, lambda a,b: mean_absolute_error(a,b))\n",
    "rmse_mean, (rmse_lo, rmse_hi) = bootstrap_ci(y_test.values, y_pred, lambda a,b: np.sqrt(mean_squared_error(a,b)))\n",
    "r2_mean, (r2_lo, r2_hi) = bootstrap_ci(y_test.values, y_pred, lambda a,b: r2_score(a,b))\n",
    "\n",
    "print(f\"Champion MAE:  {mae_mean:.3f} [{mae_lo:.3f}, {mae_hi:.3f}]  (~${to_usd(mae_mean):,.0f})\")\n",
    "print(f\"Champion RMSE: {rmse_mean:.3f} [{rmse_lo:.3f}, {rmse_hi:.3f}]\")\n",
    "print(f\"Champion R²:   {r2_mean:.3f} [{r2_lo:.3f}, {r2_hi:.3f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3d03e4",
   "metadata": {},
   "source": [
    "finished"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a0367b",
   "metadata": {},
   "source": [
    "\n",
    "## 14) Diagnostics & Calibration\n",
    "Residual histograms, residuals vs. fitted and latitude, and a reliability curve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87083c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "res = y_test - y_pred\n",
    "yp  = y_pred\n",
    "\n",
    "# Residual histogram\n",
    "plt.figure(); plt.hist(res, bins=50)\n",
    "plt.title(\"Residuals (y_true - y_pred)\"); plt.xlabel(\"residual\"); plt.ylabel(\"count\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "# Residuals vs fitted\n",
    "plt.figure(); plt.scatter(yp, res, s=6, alpha=0.5)\n",
    "plt.axhline(0, ls='--'); plt.xlabel(\"predicted\"); plt.ylabel(\"residual\"); plt.title(\"Residuals vs Predicted\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Residuals vs latitude (spatial drift)\n",
    "plt.figure(); plt.scatter(X_test['latitude'], res, s=6, alpha=0.5)\n",
    "plt.axhline(0, ls='--'); plt.xlabel(\"latitude\"); plt.ylabel(\"residual\"); plt.title(\"Residuals vs Latitude\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Reliability curve\n",
    "bins = np.quantile(yp, np.linspace(0,1,11))\n",
    "cats = pd.cut(yp, bins=bins, include_lowest=True)\n",
    "rel = pd.DataFrame({'y_true': y_test, 'y_pred': yp, 'bin': cats}).groupby('bin').mean()\n",
    "plt.figure(); plt.plot(rel['y_pred'].values, rel['y_true'].values, marker='o')\n",
    "mn, mx = rel.to_numpy().min(), rel.to_numpy().max()\n",
    "plt.plot([mn, mx], [mn, mx], ls='--')\n",
    "plt.xlabel(\"mean predicted\"); plt.ylabel(\"mean actual\"); plt.title(\"Reliability (Regression)\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6654bc",
   "metadata": {},
   "source": [
    "finished"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f220fb5",
   "metadata": {},
   "source": [
    "\n",
    "## 15) Slice Metrics (error parity)\n",
    "Error by `ocean_proximity` and by income deciles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b99dbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mae_rmse(y_true, y_hat):\n",
    "    return pd.Series({'MAE': mean_absolute_error(y_true, y_hat),\n",
    "                      'RMSE': sqrt(mean_squared_error(y_true, y_hat))})\n",
    "\n",
    "# By ocean proximity\n",
    "slice_tbl = (pd.DataFrame({'y': y_test, 'yhat': y_pred, 'grp': X_test['ocean_proximity'].astype('category')})\n",
    "             .groupby('grp').apply(lambda g: mae_rmse(g['y'], g['yhat'])))\n",
    "display(slice_tbl)\n",
    "\n",
    "# By income decile\n",
    "dec = pd.qcut(y_test, 10, labels=False, duplicates='drop')\n",
    "dec_tbl = (pd.DataFrame({'y': y_test, 'yhat': y_pred, 'dec': dec})\n",
    "           .groupby('dec').apply(lambda g: mae_rmse(g['y'], g['yhat'])))\n",
    "display(dec_tbl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118bd2a6",
   "metadata": {},
   "source": [
    "finished"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618a7efb",
   "metadata": {},
   "source": [
    "\n",
    "## 16) Explainability: Permutation Importance (test set) and optional SHAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2219d2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "def find_column_transformer(pl):\n",
    "    # Try common step names\n",
    "    for name in ['X', 'feat']:\n",
    "        if name in getattr(pl, 'named_steps', {}):\n",
    "            step = pl.named_steps[name]\n",
    "            if hasattr(step, 'named_steps') and 'prep' in step.named_steps:\n",
    "                return step.named_steps['prep']\n",
    "    # Fallback: search any ColumnTransformer in pipeline\n",
    "    for step_name, step_obj in getattr(pl, 'named_steps', {}).items():\n",
    "        if \"ColumnTransformer\" in type(step_obj).__name__:\n",
    "            return step_obj\n",
    "    return None\n",
    "\n",
    "prep = find_column_transformer(champion)\n",
    "feat_names = None\n",
    "if prep is not None and hasattr(prep, 'get_feature_names_out'):\n",
    "    try:\n",
    "        feat_names = prep.get_feature_names_out()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "perm = permutation_importance(champion, X_test, y_test, n_repeats=5, random_state=RANDOM_SEED,\n",
    "                              scoring='neg_mean_absolute_error')\n",
    "imp = pd.DataFrame({'feature': np.arange(len(perm.importances_mean)) if feat_names is None else feat_names,\n",
    "                    'perm_importance': perm.importances_mean}).sort_values('perm_importance', ascending=False)\n",
    "display(imp.head(20))\n",
    "\n",
    "# Bar plot\n",
    "top = imp.head(15)\n",
    "plt.figure(); plt.barh(top['feature'][::-1], top['perm_importance'][::-1])\n",
    "plt.title('Permutation importance (MAE decrease, test set)'); plt.tight_layout(); plt.show()\n",
    "\n",
    "# Optional SHAP for tree-based champions; may not run depending on pipeline structure.\n",
    "try:\n",
    "    import shap\n",
    "    # Attempt to find an underlying estimator that SHAP can handle; otherwise try the whole pipeline.\n",
    "    estimator = champion\n",
    "    sample_n = min(1000, X_test.shape[0])\n",
    "    samp_idx = np.random.RandomState(RANDOM_SEED).choice(X_test.index, size=sample_n, replace=False)\n",
    "    Xs = X_test.loc[samp_idx]\n",
    "    explainer = shap.Explainer(estimator)\n",
    "    sv = explainer(Xs)\n",
    "    shap.plots.beeswarm(sv, max_display=15)\n",
    "except Exception as e:\n",
    "    print(\"SHAP not run (optional):\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52de1f4d",
   "metadata": {},
   "source": [
    "finished"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebfae23",
   "metadata": {},
   "source": [
    "\n",
    "## 17) Summary: Gains vs Baselines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fc85a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pct_improve(a, b):  # improvement from baseline a to model b\n",
    "    return 100.0 * (a - b) / a\n",
    "\n",
    "MAE_c = mean_absolute_error(y_test, y_pred)\n",
    "MAE_d = mean_absolute_error(y_test, y_dummy)\n",
    "MAE_l = mean_absolute_error(y_test, y_lin)\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    'Model': ['Champion','Linear baseline','Dummy baseline'],\n",
    "    'MAE (~$10k)': [MAE_c, MAE_l, MAE_d],\n",
    "    'MAE ($)': [to_usd(MAE_c), to_usd(MAE_l), to_usd(MAE_d)]\n",
    "}).set_index('Model')\n",
    "\n",
    "summary.loc['Gain vs Dummy','% MAE improv']  = pct_improve(MAE_d, MAE_c)\n",
    "summary.loc['Gain vs Linear','% MAE improv'] = pct_improve(MAE_l, MAE_c)\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee49d287",
   "metadata": {},
   "source": [
    "finished"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fcf202",
   "metadata": {},
   "source": [
    "\n",
    "## 18) Save Artifacts (model, report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205a9ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, joblib, json, time\n",
    "ART_DIR = \"/content/models\"\n",
    "os.makedirs(ART_DIR, exist_ok=True)\n",
    "\n",
    "joblib.dump(champion, os.path.join(ART_DIR, \"champion_model.joblib\"))\n",
    "with open(os.path.join(ART_DIR, \"metrics_test.json\"), \"w\") as f:\n",
    "    json.dump({k: {m: float(v[m]) for m in v} for k,v in test_metrics.items()}, f, indent=2)\n",
    "\n",
    "print(\"Saved to:\", ART_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a48804",
   "metadata": {},
   "source": [
    "finished"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d59b984",
   "metadata": {},
   "source": [
    "\n",
    "## 19) Inference Example (data contract)\n",
    "Inputs required (columns):  \n",
    "`longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, ocean_proximity`  \n",
    "Output: predicted `median_income` (~$10k) and dollars.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e95921",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample = X_test.iloc[[0]].copy()\n",
    "pred_unit = float(champion.predict(sample)[0])\n",
    "print(\"Predicted median_income (~$10k):\", round(pred_unit, 3))\n",
    "print(\"Predicted median_income ($):\", f\"${to_usd(pred_unit):,.0f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35ddccd",
   "metadata": {},
   "source": [
    "finished"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c45a336",
   "metadata": {},
   "source": [
    "\n",
    "## Appendix — Data Dictionary (selected)\n",
    "- **longitude, latitude**: location (float)\n",
    "- **housing_median_age**: median age of houses (years)\n",
    "- **total_rooms, total_bedrooms**: stock counts\n",
    "- **population, households**: demographic counts\n",
    "- **ocean_proximity**: categorical (`<1H OCEAN`, `INLAND`, `ISLAND`, `NEAR BAY`, `NEAR OCEAN`)\n",
    "- **median_income**: target (~$10k of 1990 USD)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7255344",
   "metadata": {},
   "source": [
    "finished"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "California_Income_Prediction_CRISPDM.ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}