{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "Airbnb_KDD_End_to_End.ipynb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Airbnb Listings & Reviews — End-to-End KDD/EDA & Modeling\n\n**Author:** Your Name • **Last updated:** 2025-11-11\n\nThis notebook walks through a principled, **textbook-quality** KDD process on the Kaggle dataset **“Airbnb Listings & Reviews”**: data understanding, cleaning/preprocessing, exploratory analysis, outlier handling, feature selection, clustering, and supervised modeling to **predict listing income**. We optimize on log-revenue for stability, report metrics in $ on the natural scale, and compare strong baselines to tree ensembles.\n\n> **Compute note:** The notebook is designed to run on Colab with **limited compute**. We use sampling for heavy steps (text, permutation importance, clustering diagnostics) and keep models efficient.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 0. Environment setup & data access"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "You can provide data in one of three ways:\n1. **Kaggle API** (recommended): upload your `kaggle.json` (Account → Create New API Token) to Colab `/root/.kaggle/kaggle.json`.\n2. **Manually**: upload the extracted dataset folder to `/content/data/airbnb-listings-reviews`.\n3. **Google Drive**: mount Drive and point `DATA_DIR` accordingly.\n\n> This notebook auto-detects files like `listings*.csv`, `reviews*.csv`, and `calendar*.csv` inside `DATA_DIR`."
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {}
      },
      "source": "\n# If using Kaggle API, run this cell once you have uploaded kaggle.json via the Colab file browser.\n# It will download the dataset into /content/data/airbnb-listings-reviews\n# You may skip/ignore if you plan to provide files manually.\nimport os, subprocess, json, pathlib, shutil\n\nDATA_DIR = pathlib.Path(\"/content/data/airbnb-listings-reviews\")  # <-- Change if needed\nDATA_DIR.mkdir(parents=True, exist_ok=True)\n\ndef setup_kaggle_and_download():\n    kaggle_json = \"/root/.kaggle/kaggle.json\"\n    if not os.path.exists(kaggle_json):\n        print(\"kaggle.json not found at /root/.kaggle/kaggle.json — please upload it first.\")\n        return\n    os.chmod(\"/root/.kaggle/kaggle.json\", 0o600)\n    print(\"Installing Kaggle and downloading dataset...\")\n    subprocess.run([\"pip\",\"install\",\"-q\",\"kaggle\"], check=False)\n    # Dataset: https://www.kaggle.com/datasets/mysarahmadbhat/airbnb-listings-reviews\n    subprocess.run([\n        \"kaggle\",\"datasets\",\"download\",\"-d\",\"mysarahmadbhat/airbnb-listings-reviews\",\n        \"-p\", str(DATA_DIR)\n    ], check=False)\n    # Unzip\n    for f in os.listdir(DATA_DIR):\n        if f.endswith(\".zip\"):\n            subprocess.run([\"unzip\",\"-o\", str(DATA_DIR/f), \"-d\", str(DATA_DIR)], check=False)\n\n# Uncomment to use:\n# setup_kaggle_and_download()\n\nprint(\"DATA_DIR =\", DATA_DIR)\nprint(\"Place the dataset CSV files inside the path above if you are not using Kaggle API.\")\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {}
      },
      "source": "\n# Core packages (Colab has most preinstalled; we pin minimal versions for compatibility)\n%pip -q install -U pandas numpy scikit-learn matplotlib joblib textblob\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Imports & global configuration"
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {}
      },
      "source": "\nimport os, gc, re, json, math, warnings, textwrap, random\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom collections import Counter\n\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = (7,4)\nplt.rcParams['axes.grid'] = True\n\nfrom sklearn.model_selection import train_test_split, GroupKFold, KFold\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.ensemble import IsolationForest\n\nSEED = 42\nrnd = np.random.RandomState(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\n\nDATA_DIR = Path(\"/content/data/airbnb-listings-reviews\")  # adjust if needed\nprint(\"Using DATA_DIR:\", DATA_DIR)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Helper functions"
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {}
      },
      "source": "\ndef first_existing(dirpath: Path, names):\n    for n in names:\n        p = dirpath / n\n        if p.exists():\n            return p\n    return None\n\ndef to_money(s):\n    if pd.isna(s): return np.nan\n    return pd.to_numeric(str(s).replace(\"$\",\"\").replace(\",\",\"\").strip(), errors=\"coerce\")\n\ndef schema_report(df, name):\n    print(f\"==== {name} SHAPE {df.shape} ====\")\n    head = df.head(3)\n    cols = pd.DataFrame({\n        \"column\": df.columns,\n        \"dtype\": [df[c].dtype for c in df.columns],\n        \"null_pct\": [df[c].isna().mean().round(4) for c in df.columns],\n        \"n_unique\": [df[c].nunique(dropna=True) for c in df.columns]\n    })\n    return head, cols.sort_values([\"null_pct\",\"n_unique\"], ascending=[False, True])\n\ndef pct_out_of_bounds(s, low, high):\n    return round(100*((s < low) | (s > high)).mean(),3)\n\ndef winsorize(s, p=0.01):\n    lo, hi = s.quantile([p, 1-p])\n    return s.clip(lo, hi)\n\ndef iqr_cap(s, k=3.0):\n    q1, q3 = s.quantile([.25,.75])\n    lo, hi = q1 - k*(q3-q1), q3 + k*(q3-q1)\n    return s.clip(lo, hi)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. File discovery & initial load (lightweight)"
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {}
      },
      "source": "\ncandidates = {\n    \"listings\": [\"listings.csv\",\"listings_clean.csv\",\"listings_summary.csv\",\"listings_cities.csv\"],\n    \"reviews\":  [\"reviews.csv\",\"reviews_clean.csv\"],\n    \"calendar\": [\"calendar.csv\",\"calendar_summary.csv\"]\n}\n\nfp_listings = first_existing(DATA_DIR, candidates[\"listings\"])\nfp_reviews  = first_existing(DATA_DIR, candidates[\"reviews\"])\nfp_calendar = first_existing(DATA_DIR, candidates[\"calendar\"])\n\nprint(\"Found files:\")\nprint(\" listings:\", fp_listings)\nprint(\" reviews :\", fp_reviews)\nprint(\" calendar:\", fp_calendar)\n\nassert fp_listings is not None, \"Listings CSV not found. Please check DATA_DIR and file names.\"\n\npreview = pd.read_csv(fp_listings, nrows=500, low_memory=False)\nprint(\"Listings columns preview:\", preview.columns.tolist())\ndel preview; gc.collect();\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Load listings & basic typing / normalization"
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {}
      },
      "source": "\nlistings = pd.read_csv(fp_listings, low_memory=False)\n\ncast_cats = [\"city\",\"neighbourhood\",\"room_type\",\"property_type\",\"instant_bookable\",\"host_is_superhost\"]\nmoney_cols = [\"price\",\"cleaning_fee\",\"security_deposit\"]\n\nif \"id\" in listings.columns:\n    listings[\"id\"] = pd.to_numeric(listings[\"id\"], errors=\"coerce\").astype(\"Int64\")\n\nfor c in cast_cats:\n    if c in listings.columns:\n        listings[c] = listings[c].astype(\"category\")\n\nfor c in money_cols:\n    if c in listings.columns:\n        listings[c+\"_num\"] = listings[c].apply(to_money)\n\nfor c in [\"instant_bookable\",\"host_is_superhost\"]:\n    if c in listings.columns:\n        listings[c] = listings[c].astype(str).str.lower().map({\"t\":True,\"true\":True,\"f\":False,\"false\":False}).astype(\"boolean\")\n\nfor c in [\"last_review\",\"host_since\",\"first_review\",\"last_scraped\"]:\n    if c in listings.columns:\n        listings[c] = pd.to_datetime(listings[c], errors=\"coerce\")\n\nlistings = listings.dropna(subset=[\"id\"]).copy()\nlistings[\"id\"] = listings[\"id\"].astype(int)\nif \"last_scraped\" in listings.columns:\n    listings = listings.sort_values([\"id\",\"last_scraped\"], ascending=[True, False])\nlistings = listings.drop_duplicates(subset=[\"id\"], keep=\"first\").reset_index(drop=True)\n\nhead_listings, dict_listings = schema_report(listings, \"LISTINGS\")\ndisplay(head_listings)\ndisplay(dict_listings.head(20))\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Sanity checks (ranges & keys)"
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {}
      },
      "source": "\nnum_report = {}\n\nif {\"latitude\",\"longitude\"}.issubset(listings.columns):\n    num_report[\"lat_oob%\"] = pct_out_of_bounds(listings[\"latitude\"], -90, 90)\n    num_report[\"lon_oob%\"] = pct_out_of_bounds(listings[\"longitude\"], -180, 180)\n\nif \"price\" in listings.columns and \"price_num\" not in listings.columns:\n    listings[\"price_num\"] = listings[\"price\"].apply(to_money)\nelif \"price_num\" not in listings.columns and \"price\" not in listings.columns:\n    listings[\"price_num\"] = np.nan\n\nbounds = {\n    \"price_num\": (0, 10000),\n    \"minimum_nights\": (1, 365),\n    \"accommodates\": (1, 16)\n}\n\nfor col, (lo, hi) in bounds.items():\n    if col in listings.columns:\n        num_report[f\"{col}_oob%\"] = pct_out_of_bounds(listings[col], lo, hi)\n\nnum_report\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. Optional light load for reviews/calendar"
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {}
      },
      "source": "\nreviews = None\ncalendar = None\n\nif fp_reviews is not None:\n    try:\n        reviews = pd.read_csv(fp_reviews, usecols=[c for c in [\"listing_id\",\"date\"] if c], parse_dates=[\"date\"], low_memory=False)\n        print(\"Loaded reviews shape:\", reviews.shape)\n    except Exception as e:\n        print(\"Skipping reviews load:\", e)\n\nif fp_calendar is not None:\n    try:\n        calendar = pd.read_csv(fp_calendar, usecols=[c for c in [\"listing_id\",\"date\",\"available\",\"price\"] if c], parse_dates=[\"date\"], low_memory=False)\n        print(\"Loaded calendar shape:\", calendar.shape)\n    except Exception as e:\n        print(\"Skipping calendar load:\", e)\n\nif reviews is not None and \"listing_id\" in reviews and \"id\" in listings:\n    orphan_r = (~reviews[\"listing_id\"].isin(listings[\"id\"])).mean()\n    print(\"Orphan reviews (%):\", round(100*orphan_r,3))\n\nif calendar is not None and \"listing_id\" in calendar and \"id\" in listings:\n    orphan_c = (~calendar[\"listing_id\"].isin(listings[\"id\"])).mean()\n    print(\"Orphan calendar (%):\", round(100*orphan_c,3))\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7. Target creation: monthly revenue (calendar if present, else availability proxy)"
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {}
      },
      "source": "\ntarget = None\n\nif calendar is not None and set([\"listing_id\",\"date\",\"available\"]).issubset(calendar.columns):\n    cal = calendar.copy()\n    if \"price\" in cal.columns:\n        cal[\"price_num\"] = cal[\"price\"].apply(to_money)\n    else:\n        cal = cal.merge(listings[[\"id\",\"price_num\"]].rename(columns={\"id\":\"listing_id\"}), on=\"listing_id\", how=\"left\")\n        cal[\"price_num\"] = cal[\"price_num\"].fillna(cal[\"price_num\"].median())\n\n    cal[\"booked\"] = (cal[\"available\"].astype(str).str.lower() == \"f\").astype(int)\n    end_date = cal[\"date\"].max()\n    start_date = end_date - pd.Timedelta(days=29)\n    cal_30 = cal[(cal[\"date\"]>=start_date)&(cal[\"date\"]<=end_date)].copy()\n    cal_30[\"rev_day\"] = cal_30[\"booked\"] * cal_30[\"price_num\"]\n    target = (cal_30.groupby(\"listing_id\")[\"rev_day\"].sum()\n              .rename(\"monthly_revenue\").reset_index()\n              .rename(columns={\"listing_id\":\"id\"}))\n\nelif {\"price_num\",\"availability_30\"}.issubset(listings.columns):\n    proxy = listings[[\"id\",\"price_num\",\"availability_30\"]].copy()\n    proxy[\"availability_30\"] = proxy[\"availability_30\"].clip(0,30)\n    proxy[\"monthly_revenue\"] = proxy[\"price_num\"] * (30 - proxy[\"availability_30\"])\n    target = proxy[[\"id\",\"monthly_revenue\"]]\n\nif target is None:\n    raise RuntimeError(\"Could not construct target. Need calendar or (price_num & availability_30).\")\n\nlistings = listings.merge(target, on=\"id\", how=\"left\")\nlistings[\"y_log\"] = np.log1p(listings[\"monthly_revenue\"])\n\nprint(\"Target coverage (non-null y):\", (~listings[\"y_log\"].isna()).mean())\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8. Imputation & outlier handling (light-touch)"
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {}
      },
      "source": "\nnum_cols = [c for c in [\n    \"accommodates\",\"bathrooms\",\"bedrooms\",\"beds\",\"price_num\",\"cleaning_fee_num\",\n    \"security_deposit_num\",\"minimum_nights\",\"availability_30\",\"availability_60\",\"availability_365\",\n    \"number_of_reviews\",\"review_scores_rating\"\n] if c in listings.columns]\n\nfor c in [\"review_scores_rating\"]:\n    if c in listings.columns:\n        listings[c+\"_was_missing\"] = listings[c].isna().astype(int)\n\nif \"cleaning_fee_num\" in listings.columns:\n    listings[\"cleaning_fee_num\"] = listings[\"cleaning_fee_num\"].fillna(0)\n\nfor c in num_cols:\n    if c in listings.columns:\n        listings[c] = listings[c].fillna(listings[c].median())\n\nfor c in [\"price_num\",\"minimum_nights\",\"accommodates\",\"beds\",\"bedrooms\",\"bathrooms\",\"cleaning_fee_num\",\"number_of_reviews\"]:\n    if c in listings.columns:\n        listings[c] = winsorize(listings[c], 0.01)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 9. Amenities & lightweight text features"
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {}
      },
      "source": "\ntop_amenities = []\nif \"amenities\" in listings.columns:\n    def split_amen(s):\n        if pd.isna(s): return []\n        s = re.sub(r'[\\{\\}\\[\\]\\\"]','', str(s))\n        return [t.strip().lower() for t in s.split(\",\") if t.strip()]\n\n    amen_lists = listings[\"amenities\"].map(split_amen)\n    top_amenities = pd.Series(Counter([a for lst in amen_lists for a in lst])).sort_values(ascending=False).head(30).index.tolist()\n    for a in top_amenities:\n        col = \"amen__\" + re.sub('[^a-z0-9]+','_',a)\n        listings[col] = amen_lists.apply(lambda lst: int(a in lst))\n\nfor c in [\"name\",\"description\"]:\n    if c in listings.columns:\n        listings[f\"{c}_len_chars\"] = listings[c].astype(str).str.len()\n        listings[f\"{c}_len_words\"] = listings[c].astype(str).str.split().map(len)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 10. Geography bins (coarse)"
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {}
      },
      "source": "\nif {\"latitude\",\"longitude\"}.issubset(listings.columns):\n    listings[\"lat_bin\"] = listings[\"latitude\"].round(2).astype(str)\n    listings[\"lon_bin\"] = listings[\"longitude\"].round(2).astype(str)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 11. Build modeling table"
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {}
      },
      "source": "\nfeatures_num = [c for c in [\n    \"accommodates\",\"bathrooms\",\"bedrooms\",\"beds\",\"price_num\",\"cleaning_fee_num\",\n    \"security_deposit_num\",\"minimum_nights\",\"availability_30\",\"availability_60\",\"availability_365\",\n    \"number_of_reviews\",\"review_scores_rating\",\"name_len_chars\",\"name_len_words\",\n    \"description_len_chars\",\"description_len_words\"\n] if c in listings.columns]\n\namen_cols = [c for c in listings.columns if c.startswith(\"amen__\")]\ngeo_cols  = [c for c in [\"lat_bin\",\"lon_bin\"] if c in listings.columns]\ncat_cols  = [c for c in [\"city\",\"neighbourhood\",\"room_type\",\"property_type\",\"instant_bookable\",\"host_is_superhost\"] if c in listings.columns] + geo_cols\n\nmodel_cols = [\"id\",\"monthly_revenue\",\"y_log\"] + features_num + cat_cols + amen_cols\nmodel_df = listings[[c for c in model_cols if c in listings.columns]].dropna(subset=[\"y_log\"]).reset_index(drop=True)\nprint(\"Modeling table shape:\", model_df.shape)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 12. Quick EDA snapshots (lightweight)"
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {}
      },
      "source": "\neda_df = model_df.sample(min(len(model_df), 150_000), random_state=42)\nprint(\"Zero-share (revenue == 0):\", (eda_df[\"monthly_revenue\"]<=0).mean())\n\nplt.hist(eda_df[\"y_log\"], bins=50)\nplt.title(\"Distribution of log1p(monthly_revenue)\")\nplt.xlabel(\"y_log\")\nplt.ylabel(\"count\")\nplt.show()\n\nnum_cols_show = [c for c in [\"price_num\",\"accommodates\",\"minimum_nights\",\"beds\",\"bedrooms\",\n                             \"number_of_reviews\",\"review_scores_rating\",\"cleaning_fee_num\",\n                             \"availability_30\",\"availability_60\",\"availability_365\"]\n                 if c in eda_df.columns]\ncorrs = eda_df[num_cols_show + [\"y_log\"]].corr(numeric_only=True)[\"y_log\"].sort_values(ascending=False)\ndisplay(corrs.to_frame(\"corr_with_ylog\").head(15))\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 13. Optional: multi-dimensional outlier flag"
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {}
      },
      "source": "\nnum_probe = [c for c in [\"price_num\",\"accommodates\",\"minimum_nights\",\"beds\",\"bedrooms\",\"bathrooms\",\n                         \"cleaning_fee_num\",\"number_of_reviews\",\"availability_30\",\"availability_60\",\"availability_365\"]\n             if c in model_df.columns]\ntry:\n    sub = model_df.sample(min(30000, len(model_df)), random_state=42)\n    X_iso = sub[[c for c in num_probe if model_df[c].dtype.kind in \"fi\"]].fillna(0)\n    iso = IsolationForest(n_estimators=100, contamination=0.01, random_state=42)\n    sub[\"iso_flag\"] = (iso.fit_predict(X_iso) == -1).astype(int)\n    iso_map = dict(zip(sub[\"id\"], sub[\"iso_flag\"]))\n    model_df[\"is_outlier_modelspace\"] = model_df[\"id\"].map(iso_map).fillna(0).astype(int)\nexcept Exception as e:\n    print(\"Skipping IsolationForest:\", e)\n    model_df[\"is_outlier_modelspace\"] = 0\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 14. Feature selection (filter → MI → permutation)"
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {}
      },
      "source": "\n# Low-variance amenity dummies\nlow_var_drop = []\nfor c in [col for col in model_df.columns if col.startswith(\"amen__\")]:\n    p = model_df[c].mean()\n    if p < 0.001 or p > 0.999:\n        low_var_drop.append(c)\n\nmodel_df = model_df.drop(columns=low_var_drop)\namen_cols = [c for c in model_df.columns if c.startswith(\"amen__\")]\n\nmi_cols = [c for c in model_df.columns if (c in num_probe) or (c in amen_cols) or (c==\"is_outlier_modelspace\")]\nX_mi = model_df[mi_cols].fillna(0).astype(float)\ny_mi = model_df[\"y_log\"].values\nmi = mutual_info_regression(X_mi, y_mi, random_state=42)\nmi_rank = pd.DataFrame({\"feature\": mi_cols, \"mi\": mi}).sort_values(\"mi\", ascending=False)\ndisplay(mi_rank.head(25))\n\nkeep_mi = set(mi_rank.head(50)[\"feature\"])\n\ncat_cols  = [c for c in [\"city\",\"neighbourhood\",\"room_type\",\"property_type\",\"instant_bookable\",\"host_is_superhost\",\"lat_bin\",\"lon_bin\"] if c in model_df.columns]\nnum_cols_final = sorted(set(num_probe + [\"description_len_chars\",\"description_len_words\",\"name_len_chars\",\"name_len_words\",\"is_outlier_modelspace\"]).intersection(model_df.columns))\n\nFINAL_NUM = [c for c in num_cols_final if c in keep_mi.union(set(num_cols_final))]\nFINAL_CAT = cat_cols\ntop_amen_by_mi = mi_rank[mi_rank[\"feature\"].isin(amen_cols)].head(20)[\"feature\"].tolist()\nFINAL_BIN = top_amen_by_mi\n\nFINAL_FEATURES = FINAL_NUM + FINAL_CAT + FINAL_BIN\nprint(\"Final features (#):\", len(FINAL_FEATURES))\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 15. Clustering for segments (MiniBatchKMeans)"
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {}
      },
      "source": "\nbasic_amen = [\"amen__wifi\",\"amen__kitchen\",\"amen__washer\",\"amen__air_conditioning\",\"amen__heating\",\"amen__tv\",\"amen__self_check_in\"]\nfor a in basic_amen:\n    if a not in model_df.columns: model_df[a] = 0\nmodel_df[\"amen_count_basic\"] = model_df[basic_amen].sum(axis=1)\n\ncluster_num = [c for c in [\n    \"price_num\",\"accommodates\",\"bedrooms\",\"bathrooms\",\"minimum_nights\",\n    \"availability_30\",\"number_of_reviews\",\"review_scores_rating\",\n    \"cleaning_fee_num\",\"amen_count_basic\"\n] if c in model_df.columns]\n\nfrom sklearn.preprocessing import StandardScaler\nXc = model_df[cluster_num].copy().fillna(model_df[cluster_num].median())\nscaler_c = StandardScaler()\nZ = scaler_c.fit_transform(Xc)\n\nfrom sklearn.metrics import silhouette_score\nidx = rnd.choice(len(Z), size=min(50000, len(Z)), replace=False)\nZ_sub = Z[idx]\n\ncand_K = [3,4,5,6,8,10]\nsil = {}; inertia = {}\nfor k in cand_K:\n    km = MiniBatchKMeans(n_clusters=k, random_state=42, batch_size=2048)\n    km.fit(Z_sub)\n    inertia[k] = km.inertia_\n    s_idx = rnd.choice(len(Z_sub), size=min(15000, len(Z_sub)), replace=False)\n    sil[k] = silhouette_score(Z_sub[s_idx], km.predict(Z_sub[s_idx]))\n\nbest_k = max(sil, key=sil.get)\nprint(\"Silhouette by K:\", sil)\nprint(\"Chosen K:\", best_k)\n\nkmeans = MiniBatchKMeans(n_clusters=best_k, random_state=42, batch_size=4096)\nmodel_df[\"cluster_id\"] = kmeans.fit_predict(Z)\n\ndef profile_clusters(df, by=\"cluster_id\"):\n    num_stats = (df.groupby(by)[cluster_num + [\"monthly_revenue\",\"y_log\"]]\n                   .median()\n                   .add_prefix(\"med_\"))\n    prop_room = (pd.crosstab(df[by], df.get(\"room_type\",\"Unknown\"), normalize=\"index\")\n                   .add_prefix(\"prop_room__\"))\n    n = df.groupby(by).size().to_frame(\"n\")\n    return pd.concat([n, num_stats, prop_room], axis=1).sort_values(\"med_monthly_revenue\", ascending=False)\n\ncluster_profile = profile_clusters(model_df)\ndisplay(cluster_profile)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 16. Preprocessors (linear vs tree-friendly)"
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {}
      },
      "source": "\npre_linear = ColumnTransformer([\n    (\"num\", StandardScaler(), [c for c in FINAL_NUM + FINAL_BIN if c in model_df.columns]),\n    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", min_frequency=50), [c for c in FINAL_CAT if c in model_df.columns]),\n], remainder=\"drop\")\n\npre_tree = ColumnTransformer([\n    (\"num\", \"passthrough\", [c for c in FINAL_NUM + FINAL_BIN if c in model_df.columns]),\n    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", min_frequency=50), [c for c in FINAL_CAT if c in model_df.columns]),\n], remainder=\"drop\")\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 17. Modeling & evaluation (baselines → Ridge → HGB → RF)"
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {}
      },
      "source": "\ntarget = \"y_log\"\ngroups = model_df[\"city\"].astype(str) if \"city\" in model_df.columns else None\n\nX_cols = [c for c in FINAL_FEATURES if c in model_df.columns]\nX = model_df[X_cols]; y = model_df[target]\nids = model_df[\"id\"]\n\ndef eval_metrics(y_true_log, y_pred_log, y_true_nat):\n    y_pred_nat = np.expm1(y_pred_log)\n    rmse = mean_squared_error(y_true_nat, y_pred_nat, squared=False)\n    mae  = mean_absolute_error(y_true_nat, y_pred_nat)\n    r2   = r2_score(y_true_log, y_pred_log)\n    return {\"RMSE_$\": rmse, \"MAE_$\": mae, \"R2_log\": r2}\n\ngkf = GroupKFold(n_splits=5) if groups is not None else KFold(n_splits=5, shuffle=True, random_state=42)\nresults = []\n\n# Baselines\nfor fold, (tr, va) in enumerate(gkf.split(X, y, groups=groups)):\n    y_tr, y_va = y.iloc[tr], y.iloc[va]\n    y_va_nat = np.expm1(y_va)\n\n    pred_null = np.full_like(y_va, y_tr.mean(), dtype=float)\n    res_null = eval_metrics(y_va, pred_null, y_va_nat)\n    results.append({\"model\":\"NULL_global\",\"fold\":fold, **res_null})\n\n    if groups is not None:\n        city_tr_mean = y.iloc[tr].groupby(groups.iloc[tr]).mean()\n        pred_city = groups.iloc[va].map(city_tr_mean).fillna(y_tr.mean()).values\n        res_city = eval_metrics(y_va, pred_city, y_va_nat)\n        results.append({\"model\":\"BASE_city_mean\",\"fold\":fold, **res_city})\n\n# Ridge\nridge = Pipeline([(\"pre\", pre_linear), (\"est\", Ridge(alpha=1.0, random_state=42))])\nfor fold, (tr, va) in enumerate(gkf.split(X, y, groups=groups)):\n    ridge.fit(X.iloc[tr], y.iloc[tr])\n    pred = ridge.predict(X.iloc[va])\n    res = eval_metrics(y.iloc[va], pred, np.expm1(y.iloc[va]))\n    results.append({\"model\":\"Ridge(a=1.0)\",\"fold\":fold, **res})\n\n# HGB\nhgb = Pipeline([(\"pre\", pre_tree), (\"est\", HistGradientBoostingRegressor(\n    learning_rate=0.08, max_leaf_nodes=31, random_state=42))])\nfor fold, (tr, va) in enumerate(gkf.split(X, y, groups=groups)):\n    hgb.fit(X.iloc[tr], y.iloc[tr])\n    pred = hgb.predict(X.iloc[va])\n    res = eval_metrics(y.iloc[va], pred, np.expm1(y.iloc[va]))\n    results.append({\"model\":\"HGB\",\"fold\":fold, **res})\n\n# RF\nrf = Pipeline([(\"pre\", pre_tree), (\"est\", RandomForestRegressor(\n    n_estimators=200, max_depth=18, min_samples_leaf=5, n_jobs=-1, random_state=42))])\nfor fold, (tr, va) in enumerate(gkf.split(X, y, groups=groups)):\n    rf.fit(X.iloc[tr], y.iloc[tr])\n    pred = rf.predict(X.iloc[va])\n    res = eval_metrics(y.iloc[va], pred, np.expm1(y.iloc[va]))\n    results.append({\"model\":\"RF\",\"fold\":fold, **res})\n\nres_df = pd.DataFrame(results)\ndisplay(res_df.groupby(\"model\")[[\"RMSE_$\",\"MAE_$\",\"R2_log\"]].agg([\"mean\",\"std\"]).sort_values((\"RMSE_$\",\"mean\")))\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 18. Out-of-fold predictions & by-city report"
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {}
      },
      "source": "\n# OOF with HGB\noof_pred = np.zeros(len(model_df))\nfor tr, va in gkf.split(X, y, groups=groups):\n    hgb.fit(X.iloc[tr], y.iloc[tr])\n    oof_pred[va] = hgb.predict(X.iloc[va])\n\noof = pd.DataFrame({\n    \"id\": ids,\n    \"city\": groups if groups is not None else \"ALL\",\n    \"y_log_true\": y,\n    \"y_log_pred\": oof_pred,\n    \"rev_true\": np.expm1(y),\n    \"rev_pred\": np.expm1(oof_pred),\n})\ncity_report = (oof.groupby(\"city\")\n                  .apply(lambda g: pd.Series({\n                      \"RMSE_$\": mean_squared_error(g[\"rev_true\"], g[\"rev_pred\"], squared=False),\n                      \"MAE_$\": mean_absolute_error(g[\"rev_true\"], g[\"rev_pred\"]),\n                      \"R2_log\": r2_score(g[\"y_log_true\"], g[\"y_log_pred\"]),\n                      \"n\": len(g)\n                  }))\n                  .sort_values(\"RMSE_$\"))\ndisplay(city_report.head(20))\n\noof[\"resid_log\"] = oof[\"y_log_true\"] - oof[\"y_log_pred\"]\nprint(\"Mean residual (log-scale):\", oof[\"resid_log\"].mean())\nplt.hist(oof[\"resid_log\"], bins=50); plt.title(\"Residuals (log-scale)\"); plt.show()\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 19. Permutation importance (snapshot)"
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {}
      },
      "source": "\nval_idx = rnd.choice(len(X), size=min(20000, len(X)), replace=False)\nhgb.fit(X.iloc[~np.isin(np.arange(len(X)), val_idx)], y.iloc[~np.isin(np.arange(len(X)), val_idx)])\npi = permutation_importance(hgb, X.iloc[val_idx], y.iloc[val_idx], n_repeats=5, random_state=42)\nfeat_names = hgb.named_steps[\"pre\"].get_feature_names_out()\nperm_df = pd.DataFrame({\"feature\": feat_names, \"pi\": pi.importances_mean}).sort_values(\"pi\", ascending=False).head(30)\ndisplay(perm_df)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 20. Persist artifacts (optional)"
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {}
      },
      "source": "\nfrom joblib import dump\nARTIFACTS = Path(\"/content/artifacts\"); ARTIFACTS.mkdir(exist_ok=True, parents=True)\n\ndump(hgb, ARTIFACTS/\"final_hgb_model.joblib\")\ndump(pre_linear, ARTIFACTS/\"pre_linear.joblib\")\ndump(pre_tree, ARTIFACTS/\"pre_tree.joblib\")\noof.to_parquet(ARTIFACTS/\"oof_predictions.parquet\")\ncity_report.to_csv(ARTIFACTS/\"city_report.csv\")\n\nprint(\"Saved artifacts to:\", ARTIFACTS)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 21. Executive summary & recommendations"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Key findings (confirm with your run):**\n- **Availability** (e.g., `availability_30`) is a strong negative driver of revenue.\n- **City** and **room_type** show large fixed effects; keep as categoricals.\n- **Accommodates/bedrooms** help with diminishing returns; **price** is nonlinear.\n- **Amenities** like wifi/AC/washer/self check-in are commonly beneficial where missing.\n- Unsupervised **clusters** yield actionable segments for targeted pricing/amenity strategies.\n\n**Recommendations:**\n1. For high-availability/low-revenue listings, test targeted **price reductions** and add **must-have amenities**.\n2. Lower **minimum_nights** to unlock weekend/short-stay demand where appropriate.\n3. Use **cluster_id** and city-level reports to prioritize experimentation.\n4. Extend with **text features** (TF‑IDF on descriptions/reviews) and/or **gradient boosting libraries** if compute allows.\n\n**Limitations & next steps:** seasonality/event effects, currency differences across regions, and zero-inflation (consider two-stage models)."
    }
  ]
}