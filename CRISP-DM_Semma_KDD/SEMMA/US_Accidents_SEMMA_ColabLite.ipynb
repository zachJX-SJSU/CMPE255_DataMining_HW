{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82266742",
   "metadata": {},
   "source": [
    "# US Accidents — SEMMA (Colab Lite Optimized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd6052a",
   "metadata": {},
   "source": [
    "\n",
    "**Focus:** A compute-aware SEMMA pipeline runnable on Colab free tier.\n",
    "\n",
    "**Key choices**\n",
    "- Chunked CSV sampling (≈1% pool → stratify to **50k** rows)\n",
    "- Hash encoding for high-cardinality features\n",
    "- Top-128 feature selection\n",
    "- Small ensembles and tiny tuning grids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c50c0cc",
   "metadata": {},
   "source": [
    "## 0) Environment (installs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08118d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Install minimal dependencies (quiet)\n",
    "%%capture\n",
    "import sys, subprocess, pkgutil\n",
    "def pip_install(pkg):\n",
    "    if pkg not in {m.name for m in pkgutil.iter_modules()}:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "pip_install(\"kaggle\"); pip_install(\"opendatasets\"); pip_install(\"category_encoders\"); pip_install(\"pyarrow\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4d05eb",
   "metadata": {},
   "source": [
    "## 1) Data Access & Chunked Sampling (SEMMA — S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8658032",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Download data and build a 50k stratified sample (chunked)\n",
    "from pathlib import Path\n",
    "import os, glob, shutil, numpy as np, pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "BASE = Path(\"/content\")\n",
    "DATA = BASE/\"data\"; RAW = DATA/\"raw\"; SAMPLES = DATA/\"samples\"\n",
    "for p in [DATA, RAW, SAMPLES]: p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download via opendatasets (Kaggle auth required in browser)\n",
    "try:\n",
    "    import opendatasets as od\n",
    "    od.download(\"https://www.kaggle.com/datasets/sobhanmoosavi/us-accidents\", data_dir=\"/content/us-accidents\")\n",
    "except Exception as e:\n",
    "    print(\"If download failed, ensure Kaggle TOS accepted. Error:\", e)\n",
    "\n",
    "# Pick largest US_Accidents*.csv\n",
    "cands = sorted(glob.glob(\"/content/us-accidents/**/*.csv\", recursive=True)) + sorted(glob.glob(\"/content/us-accidents/*.csv\"))\n",
    "CSV = None; size = -1\n",
    "for c in cands:\n",
    "    name = os.path.basename(c)\n",
    "    if \"US_Accidents\" in name and name.lower().endswith(\".csv\"):\n",
    "        s = os.path.getsize(c)\n",
    "        if s > size: CSV, size = c, s\n",
    "assert CSV is not None, \"US_Accidents*.csv not found.\"\n",
    "shutil.copy(CSV, RAW/f\"{Path(CSV).name}\")\n",
    "print(\"Using:\", CSV)\n",
    "\n",
    "# Chunked sampling: 1% pool -> stratify to 50k\n",
    "TARGET_SAMPLE_SIZE = 50_000\n",
    "POOL_MULTIPLIER = 1.8\n",
    "POOL_TARGET = int(TARGET_SAMPLE_SIZE * POOL_MULTIPLIER)\n",
    "P_CHUNK = 0.01\n",
    "CHUNK_SIZE = 200_000\n",
    "\n",
    "usecols = [\"ID\",\"Severity\",\"Start_Time\",\"End_Time\",\"Start_Lat\",\"Start_Lng\",\"Distance(mi)\",\n",
    "           \"City\",\"County\",\"State\",\"Zipcode\",\"Timezone\",\"Temperature(F)\",\"Humidity(%)\",\"Pressure(in)\",\n",
    "           \"Visibility(mi)\",\"Wind_Speed(mph)\",\"Wind_Direction\",\"Precipitation(in)\",\"Weather_Condition\",\n",
    "           \"Amenity\",\"Bump\",\"Crossing\",\"Junction\",\"Traffic_Signal\",\"Sunrise_Sunset\"]\n",
    "dtypes = {\"ID\":\"string\",\"Severity\":\"Int8\",\"Start_Lat\":\"float32\",\"Start_Lng\":\"float32\",\"Distance(mi)\":\"float32\",\n",
    "          \"City\":\"string\",\"County\":\"string\",\"State\":\"string\",\"Zipcode\":\"string\",\"Timezone\":\"string\",\n",
    "          \"Temperature(F)\":\"float32\",\"Humidity(%)\":\"float32\",\"Pressure(in)\":\"float32\",\"Visibility(mi)\":\"float32\",\n",
    "          \"Wind_Speed(mph)\":\"float32\",\"Wind_Direction\":\"string\",\"Precipitation(in)\":\"float32\",\"Weather_Condition\":\"string\",\n",
    "          \"Amenity\":\"boolean\",\"Bump\":\"boolean\",\"Crossing\":\"boolean\",\"Junction\":\"boolean\",\"Traffic_Signal\":\"boolean\",\n",
    "          \"Sunrise_Sunset\":\"string\"}\n",
    "\n",
    "pool_parts=[]; acc=0\n",
    "reader = pd.read_csv(CSV, usecols=usecols, dtype=dtypes, parse_dates=[\"Start_Time\",\"End_Time\"], chunksize=CHUNK_SIZE)\n",
    "for i,chunk in enumerate(reader):\n",
    "    chunk = chunk.dropna(subset=[\"Severity\",\"Start_Time\",\"End_Time\"]).copy()\n",
    "    dur = (chunk[\"End_Time\"] - chunk[\"Start_Time\"]).dt.total_seconds()/60.0\n",
    "    chunk = chunk[dur >= 0].copy()\n",
    "    chunk[\"Duration_min\"] = dur[dur >= 0]\n",
    "    m = np.random.random(len(chunk)) < P_CHUNK\n",
    "    take = chunk.loc[m]\n",
    "    pool_parts.append(take); acc += len(take)\n",
    "    print(f\"Chunk {i}: took {len(take):,}, pool={acc:,}\")\n",
    "    if acc >= POOL_TARGET: break\n",
    "\n",
    "pool = pd.concat(pool_parts, ignore_index=True)\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=TARGET_SAMPLE_SIZE, random_state=42)\n",
    "_, idx = next(sss.split(pool, pool[\"Severity\"].astype(int)))\n",
    "sample = pool.iloc[idx].reset_index(drop=True)\n",
    "\n",
    "SAMPLE_PARQUET = SAMPLES/\"us_accidents_sample_50k.parquet\"\n",
    "sample.to_parquet(SAMPLE_PARQUET, index=False)\n",
    "print(\"Saved sample:\", SAMPLE_PARQUET, \" shape:\", sample.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8389870",
   "metadata": {},
   "source": [
    "## 2) Preprocessing (FeatureBuilder + ColumnTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508f8d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Build the lean preprocessing pipeline\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "class FeatureBuilder(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X[\"hour\"] = X[\"Start_Time\"].dt.hour.astype(\"int16\")\n",
    "        X[\"dow\"]  = X[\"Start_Time\"].dt.dayofweek.astype(\"int16\")\n",
    "        X[\"month\"]= X[\"Start_Time\"].dt.month.astype(\"int16\")\n",
    "        X[\"is_weekend\"] = X[\"dow\"].isin([5,6]).astype(\"int8\")\n",
    "        X[\"rush_hour\"]  = X[\"hour\"].isin([7,8,9,16,17,18]).astype(\"int8\")\n",
    "        w = X[\"Weather_Condition\"].fillna(\"Unknown\").str.lower()\n",
    "        def bucket_weather(s):\n",
    "            cat = np.full(len(s), \"other/unknown\", dtype=object)\n",
    "            m = s.str.contains\n",
    "            cat[m(r\"(snow|sleet|blizzard|flurries|wintry)\")] = \"snow\"\n",
    "            cat[m(r\"(ice|freez)\")] = \"ice\"\n",
    "            cat[m(r\"(rain|drizzle|shower|storm|thunder|t\\-?storm)\")] = \"rain/storm\"\n",
    "            cat[m(r\"(fog|mist|haze|smoke)\")] = \"fog/mist\"\n",
    "            cat[m(r\"(clear|fair)\")] = \"clear\"\n",
    "            cat[m(r\"(cloud)\")] = \"cloudy\"\n",
    "            cat[m(r\"(wind)\")] = \"windy\"\n",
    "            return pd.Series(cat, index=s.index)\n",
    "        X[\"weather_bucket\"] = bucket_weather(w).astype(\"string\")\n",
    "        X[\"precip_bucket\"] = pd.cut(X[\"Precipitation(in)\"], [-0.001,0.0,0.1,0.5,np.inf],\n",
    "                                    labels=[\"none\",\"(0,0.1]\",\"(0.1,0.5]\",\">0.5\"], include_lowest=True).astype(\"string\")\n",
    "        X[\"vis_bucket\"] = pd.cut(X[\"Visibility(mi)\"], [-0.001,1,3,5,10,np.inf],\n",
    "                                 labels=[\"<=1\",\"(1,3]\",\"(3,5]\",\"(5,10]\",\"(10,inf)\"], include_lowest=True).astype(\"string\")\n",
    "        def map_wind_dir(x):\n",
    "            if pd.isna(x): return \"Unknown\"\n",
    "            x = str(x).upper()\n",
    "            if x in {\"CALM\",\"VAR\",\"VARIABLE\",\"VRB\"}: return \"Variable\"\n",
    "            mapping = {\"N\":\"N\",\"NNE\":\"N\",\"NNW\":\"N\",\"NE\":\"NE\",\"ENE\":\"NE\",\"E\":\"E\",\"ESE\":\"E\",\n",
    "                       \"SE\":\"SE\",\"SSE\":\"SE\",\"S\":\"S\",\"SSW\":\"S\",\"SW\":\"SW\",\"WSW\":\"SW\",\"W\":\"W\",\n",
    "                       \"WNW\":\"W\",\"NW\":\"NW\",\"NNW\":\"NW\"}\n",
    "            return mapping.get(x, x if x in {\"N\",\"NE\",\"E\",\"SE\",\"S\",\"SW\",\"W\",\"NW\"} else \"Other\")\n",
    "        X[\"wind8\"] = X[\"Wind_Direction\"].map(map_wind_dir).astype(\"string\")\n",
    "        for b in [\"Amenity\",\"Bump\",\"Crossing\",\"Junction\",\"Traffic_Signal\"]:\n",
    "            X[b] = X[b].fillna(False).astype(\"int8\")\n",
    "        return X\n",
    "\n",
    "class WinsorClipper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lower=0.01, upper=0.99): self.lower=lower; self.upper=upper\n",
    "    def fit(self, X, y=None):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        self.lo_ = np.nanpercentile(X, self.lower*100, axis=0)\n",
    "        self.hi_ = np.nanpercentile(X, self.upper*100, axis=0); return self\n",
    "    def transform(self, X): return np.clip(np.asarray(X, dtype=float), self.lo_, self.hi_)\n",
    "\n",
    "num_cols = [\"Distance(mi)\",\"Temperature(F)\",\"Humidity(%)\",\"Pressure(in)\",\"Visibility(mi)\",\"Wind_Speed(mph)\",\n",
    "            \"Precipitation(in)\",\"Start_Lat\",\"Start_Lng\",\"hour\",\"dow\",\"month\",\"is_weekend\",\"rush_hour\"]\n",
    "bin_cols = [\"Amenity\",\"Bump\",\"Crossing\",\"Junction\",\"Traffic_Signal\"]\n",
    "low_card = [\"State\",\"Timezone\",\"Sunrise_Sunset\",\"wind8\",\"weather_bucket\",\"precip_bucket\",\"vis_bucket\"]\n",
    "high_card= [\"City\",\"County\",\"Zipcode\"]\n",
    "\n",
    "import category_encoders as ce\n",
    "high_cat = Pipeline([(\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                     (\"hash\", ce.HashingEncoder(n_components=2**10, drop_invariant=True, return_df=True))])\n",
    "low_cat  = Pipeline([(\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                     (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=True))])\n",
    "num_pipe = Pipeline([(\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "                     (\"winsor\", WinsorClipper(0.01,0.99)),\n",
    "                     (\"scale\", RobustScaler())])\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "preprocess = ColumnTransformer([\n",
    "    (\"num\", num_pipe, num_cols),\n",
    "    (\"bin\", \"passthrough\", bin_cols),\n",
    "    (\"lowcat\", low_cat, low_card),\n",
    "    (\"highcat\", high_cat, high_card)\n",
    "], remainder=\"drop\", sparse_threshold=0.3)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "preprocess_pipe = Pipeline([(\"features\", FeatureBuilder()), (\"preprocess\", preprocess)])\n",
    "\n",
    "print(\"Preprocess ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965a80d5",
   "metadata": {},
   "source": [
    "## 3) Feature Selection & Temporal Split (SEMMA — M2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a922aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Temporal split, MI<=15k, embedded selection -> lean 128 features\n",
    "import numpy as np, pandas as pd\n",
    "from scipy import sparse\n",
    "\n",
    "df = pd.read_parquet(\"/content/data/samples/us_accidents_sample_50k.parquet\")\n",
    "def temporal_split(df, time_col=\"Start_Time\", test_frac=0.20, valid_frac=0.10):\n",
    "    df = df.sort_values(time_col).reset_index(drop=True)\n",
    "    n = len(df); nt = int(n*test_frac); nv = int(n*valid_frac)\n",
    "    return df.iloc[:n-nt-nv].copy(), df.iloc[n-nt-nv:n-nt].copy(), df.iloc[n-nt:].copy()\n",
    "\n",
    "df_train, df_valid, df_test = temporal_split(df)\n",
    "for part in (df_train, df_valid, df_test): part[\"y_reg\"] = np.log1p(part[\"Duration_min\"])\n",
    "\n",
    "def drop_leakage(d): return d.drop(columns=[\"Severity\",\"End_Time\",\"Duration_min\",\"y_reg\"], errors=\"ignore\")\n",
    "Xc_train, yc_train = drop_leakage(df_train), df_train[\"Severity\"].astype(int).values\n",
    "Xc_valid, yc_valid = drop_leakage(df_valid), df_valid[\"Severity\"].astype(int).values\n",
    "Xc_test,  yc_test  = drop_leakage(df_test),  df_test[\"Severity\"].astype(int).values\n",
    "Xr_train, yr_train = drop_leakage(df_train), df_train[\"y_reg\"].values\n",
    "Xr_valid, yr_valid = drop_leakage(df_valid), df_valid[\"y_reg\"].values\n",
    "Xr_test,  yr_test  = drop_leakage(df_test),  df_test[\"y_reg\"].values\n",
    "\n",
    "Xc_train_t = preprocess_pipe.fit_transform(Xc_train, yc_train); Xc_valid_t = preprocess_pipe.transform(Xc_valid); Xc_test_t  = preprocess_pipe.transform(Xc_test)\n",
    "Xr_train_t = preprocess_pipe.fit_transform(Xr_train, yr_train); Xr_valid_t = preprocess_pipe.transform(Xr_valid); Xr_test_t  = preprocess_pipe.transform(Xr_test)\n",
    "\n",
    "def nzv_mask(X, thresh=1e-9):\n",
    "    if sparse.issparse(X):\n",
    "        var = np.array((X.power(2)).mean(axis=0) - np.square(X.mean(axis=0))).ravel()\n",
    "    else:\n",
    "        var = X.var(axis=0)\n",
    "    return var > thresh\n",
    "\n",
    "mask_c = nzv_mask(Xc_train_t); mask_r = nzv_mask(Xr_train_t)\n",
    "def mapply(X, mask): import numpy as np; return X[:, np.where(mask)[0]]\n",
    "Xc_train_t, Xc_valid_t, Xc_test_t = mapply(Xc_train_t, mask_c), mapply(Xc_valid_t, mask_c), mapply(Xc_test_t, mask_c)\n",
    "Xr_train_t, Xr_valid_t, Xr_test_t = mapply(Xr_train_t, mask_r), mapply(Xr_valid_t, mask_r), mapply(Xr_test_t, mask_r)\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "n_mi = min(Xc_train_t.shape[0], 15000)\n",
    "mi_c = mutual_info_classif(Xc_train_t[:n_mi], yc_train[:n_mi], random_state=42)\n",
    "mi_r = mutual_info_regression(Xr_train_t[:n_mi], yr_train[:n_mi], random_state=42)\n",
    "\n",
    "TOP_K = 128\n",
    "top_c_idx = np.argsort(mi_c)[::-1][:TOP_K]; top_r_idx = np.argsort(mi_r)[::-1][:TOP_K]\n",
    "Xc_train_mi, Xc_valid_mi, Xc_test_mi = Xc_train_t[:, top_c_idx], Xc_valid_t[:, top_c_idx], Xc_test_t[:, top_c_idx]\n",
    "Xr_train_mi, Xr_valid_mi, Xr_test_mi = Xr_train_t[:, top_r_idx], Xr_valid_t[:, top_r_idx], Xr_test_t[:, top_r_idx]\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor\n",
    "logit_l1 = LogisticRegression(penalty=\"l1\", solver=\"liblinear\", C=0.7, class_weight=\"balanced\", max_iter=200).fit(Xc_train_mi, yc_train)\n",
    "coef_c = np.mean(np.abs(logit_l1.coef_), axis=0)\n",
    "lasso = Lasso(alpha=0.001, max_iter=1500).fit(Xr_train_mi, yr_train); coef_r = np.abs(lasso.coef_)\n",
    "etc = ExtraTreesClassifier(n_estimators=100, random_state=42, class_weight=\"balanced_subsample\", n_jobs=-1).fit(Xc_train_mi, yc_train); imp_c = etc.feature_importances_\n",
    "etr = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1).fit(Xr_train_mi, yr_train); imp_r = etr.feature_importances_\n",
    "\n",
    "def top_union(*arrs, k=128):\n",
    "    ranks = [np.argsort(a)[::-1] for a in arrs]; seen=set(); out=[]\n",
    "    for r in ranks:\n",
    "        for idx in r:\n",
    "            if idx not in seen:\n",
    "                seen.add(idx); out.append(idx)\n",
    "            if len(out)>=k: break\n",
    "        if len(out)>=k: break\n",
    "    return np.array(out)\n",
    "\n",
    "lean_c_idx = top_union(mi_c[top_c_idx], imp_c, coef_c, k=128)\n",
    "lean_r_idx = top_union(mi_r[top_r_idx], imp_r, coef_r, k=128)\n",
    "Xc_train_lean, Xc_valid_lean, Xc_test_lean = Xc_train_mi[:, lean_c_idx], Xc_valid_mi[:, lean_c_idx], Xc_test_mi[:, lean_c_idx]\n",
    "Xr_train_lean, Xr_valid_lean, Xr_test_lean = Xr_train_mi[:, lean_r_idx], Xr_valid_mi[:, lean_r_idx], Xr_test_mi[:, lean_r_idx]\n",
    "\n",
    "print(\"Lean shapes — Cls:\", Xc_train_lean.shape, Xc_valid_lean.shape, Xc_test_lean.shape, \" Reg:\", Xr_train_lean.shape, Xr_valid_lean.shape, Xr_test_lean.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce94b88b",
   "metadata": {},
   "source": [
    "## 4) Modeling (baselines, tiny tuning, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a76ca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Fit baselines & small models; tiny tuning; test metrics\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score, cohen_kappa_score, confusion_matrix\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.linear_model import LogisticRegression, HuberRegressor, Ridge\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from numpy import expm1, sqrt\n",
    "from scipy.sparse import vstack, issparse\n",
    "\n",
    "def cls_metrics(y_true, y_pred):\n",
    "    return {\"balanced_acc\":balanced_accuracy_score(y_true, y_pred),\n",
    "            \"macro_f1\":f1_score(y_true, y_pred, average=\"macro\"),\n",
    "            \"qwk\":cohen_kappa_score(y_true, y_pred, weights=\"quadratic\"),\n",
    "            \"cm\":confusion_matrix(y_true, y_pred, normalize=\"true\")}\n",
    "\n",
    "def reg_metrics(y_true_log, y_pred_log, y_true_minutes):\n",
    "    mae_log = mean_absolute_error(y_true_log, y_pred_log)\n",
    "    rmse_log = sqrt(mean_squared_error(y_true_log, y_pred_log))\n",
    "    y_pred_min = expm1(y_pred_log)\n",
    "    mae_min = mean_absolute_error(y_true_minutes, y_pred_min)\n",
    "    return {\"mae_log\":mae_log, \"rmse_log\":rmse_log, \"mae_min\":mae_min}\n",
    "\n",
    "# Baselines and first models\n",
    "d_cls = DummyClassifier(strategy=\"most_frequent\").fit(Xc_train_lean, yc_train)\n",
    "d_reg = DummyRegressor(strategy=\"median\").fit(Xr_train_lean, yr_train)\n",
    "logit = LogisticRegression(max_iter=300, class_weight=\"balanced\").fit(Xc_train_lean, yc_train)\n",
    "rf_c  = RandomForestClassifier(n_estimators=200, max_depth=14, min_samples_leaf=3,\n",
    "                               class_weight=\"balanced_subsample\", n_jobs=-1, random_state=42).fit(Xc_train_lean, yc_train)\n",
    "huber = HuberRegressor(alpha=1e-4, epsilon=1.5, max_iter=400).fit(Xr_train_lean, yr_train)\n",
    "ridge = Ridge(alpha=1.0, random_state=42).fit(Xr_train_lean, yr_train)\n",
    "rf_r  = RandomForestRegressor(n_estimators=200, max_depth=16, min_samples_leaf=3,\n",
    "                              n_jobs=-1, random_state=42).fit(Xr_train_lean, yr_train)\n",
    "\n",
    "print(\"Valid — Classification:\", { \"dummy\": {k:round(v,4) for k,v in cls_metrics(yc_valid, d_cls.predict(Xc_valid_lean)).items() if k!='cm'},\n",
    "       \"logit\": {k:round(v,4) for k,v in cls_metrics(yc_valid, logit.predict(Xc_valid_lean)).items() if k!='cm'},\n",
    "       \"rf\":    {k:round(v,4) for k,v in cls_metrics(yc_valid, rf_c.predict(Xc_valid_lean)).items() if k!='cm'} })\n",
    "\n",
    "print(\"Valid — Regression:\", {\n",
    "    \"dummy\": {k:round(v,4) for k,v in reg_metrics(yr_valid, d_reg.predict(Xr_valid_lean), df_valid['Duration_min'].values).items()},\n",
    "    \"huber\": {k:round(v,4) for k,v in reg_metrics(yr_valid, huber.predict(Xr_valid_lean), df_valid['Duration_min'].values).items()},\n",
    "    \"ridge\": {k:round(v,4) for k,v in reg_metrics(yr_valid, ridge.predict(Xr_valid_lean), df_valid['Duration_min'].values).items()},\n",
    "    \"rf\":    {k:round(v,4) for k,v in reg_metrics(yr_valid, rf_r.predict(Xr_valid_lean), df_valid['Duration_min'].values).items()},\n",
    "})\n",
    "\n",
    "# Tiny tuning\n",
    "sev_grid = {\"C\":[0.7,1.0], \"rf_n\":[150,250], \"rf_d\":[12,14]}\n",
    "best_sev, best_qwk = rf_c, cohen_kappa_score(yc_valid, rf_c.predict(Xc_valid_lean), weights=\"quadratic\")\n",
    "for C in sev_grid[\"C\"]:\n",
    "    m = LogisticRegression(max_iter=350, class_weight=\"balanced\", C=C).fit(Xc_train_lean, yc_train)\n",
    "    q = cohen_kappa_score(yc_valid, m.predict(Xc_valid_lean), weights=\"quadratic\")\n",
    "    if q > best_qwk: best_sev, best_qwk = m, q\n",
    "for n in sev_grid[\"rf_n\"]:\n",
    "    for d in sev_grid[\"rf_d\"]:\n",
    "        m = RandomForestClassifier(n_estimators=n, max_depth=d, min_samples_leaf=3,\n",
    "                                   class_weight=\"balanced_subsample\", n_jobs=-1, random_state=42).fit(Xc_train_lean, yc_train)\n",
    "        q = cohen_kappa_score(yc_valid, m.predict(Xc_valid_lean), weights=\"quadratic\")\n",
    "        if q > best_qwk: best_sev, best_qwk = m, q\n",
    "\n",
    "dur_grid = {\"alpha\":[0.7,1.0,3.0], \"rf_n\":[150,250], \"rf_d\":[14,16]}\n",
    "best_dur, best_mae = rf_r, reg_metrics(yr_valid, rf_r.predict(Xr_valid_lean), df_valid['Duration_min'].values)[\"mae_min\"]\n",
    "for a in dur_grid[\"alpha\"]:\n",
    "    m = Ridge(alpha=a, random_state=42).fit(Xr_train_lean, yr_train)\n",
    "    mae = reg_metrics(yr_valid, m.predict(Xr_valid_lean), df_valid['Duration_min'].values)[\"mae_min\"]\n",
    "    if mae < best_mae: best_dur, best_mae = m, mae\n",
    "for n in dur_grid[\"rf_n\"]:\n",
    "    for d in dur_grid[\"rf_d\"]:\n",
    "        m = RandomForestRegressor(n_estimators=n, max_depth=d, min_samples_leaf=3, n_jobs=-1, random_state=42).fit(Xr_train_lean, yr_train)\n",
    "        mae = reg_metrics(yr_valid, m.predict(Xr_valid_lean), df_valid['Duration_min'].values)[\"mae_min\"]\n",
    "        if mae < best_mae: best_dur, best_mae = m, mae\n",
    "\n",
    "# Ordinal via thresholded regression (validation-only)\n",
    "def qwk_thresholds(y_true, y_score):\n",
    "    thr = np.percentile(y_score, [25,50,75]).astype(float)\n",
    "    def disc(s,t): return np.clip(1 + (s>t[0]) + (s>t[1]) + (s>t[2]), 1, 4).astype(int)\n",
    "    best = thr.copy(); best_q = cohen_kappa_score(y_true, disc(y_score,best), weights=\"quadratic\")\n",
    "    for step in [0.2,0.1,0.05,0.02]:\n",
    "        improved=True\n",
    "        while improved:\n",
    "            improved=False\n",
    "            for i in range(3):\n",
    "                for delta in [-step, step]:\n",
    "                    tr = best.copy(); tr[i]+=delta; tr=np.sort(tr)\n",
    "                    q = cohen_kappa_score(y_true, disc(y_score,tr), weights=\"quadratic\")\n",
    "                    if q>best_q: best, best_q, improved = tr, q, True\n",
    "    return best, best_q\n",
    "reg_sev = Ridge(alpha=1.0, random_state=42).fit(Xc_train_lean, yc_train)\n",
    "thr, q = qwk_thresholds(yc_valid, reg_sev.predict(Xc_valid_lean))\n",
    "print(\"Ordinal ridge QWK (valid):\", round(q,4), \" thresholds:\", thr)\n",
    "\n",
    "# Train on train+valid and evaluate on test\n",
    "def stack(X_tr, X_va, y_tr, y_va):\n",
    "    X = vstack([X_tr, X_va]) if issparse(X_tr) else np.vstack([X_tr, X_va])\n",
    "    y = np.concatenate([y_tr, y_va]); return X, y\n",
    "Xc_trva, yc_trva = stack(Xc_train_lean, Xc_valid_lean, yc_train, yc_valid)\n",
    "Xr_trva, yr_trva = stack(Xr_train_lean, Xr_valid_lean, yr_train, yr_valid)\n",
    "best_sev.fit(Xc_trva, yc_trva); best_dur.fit(Xr_trva, yr_trva)\n",
    "\n",
    "sev_pred_test = best_sev.predict(Xc_test_lean)\n",
    "sev_test = {\"qwk\": cohen_kappa_score(yc_test, sev_pred_test, weights=\"quadratic\"),\n",
    "            \"bal_acc\": balanced_accuracy_score(yc_test, sev_pred_test),\n",
    "            \"macro_f1\": f1_score(yc_test, sev_pred_test, average=\"macro\")}\n",
    "dur_pred_test_log = best_dur.predict(Xr_test_lean)\n",
    "mae_min = mean_absolute_error(df_test[\"Duration_min\"].values, expm1(dur_pred_test_log))\n",
    "mae_log = mean_absolute_error(yr_test, dur_pred_test_log)\n",
    "rmse_log = sqrt(mean_squared_error(yr_test, dur_pred_test_log))\n",
    "print(\"TEST — Severity:\", {k:round(v,4) for k,v in sev_test.items()})\n",
    "print(\"TEST — Duration:\", {\"mae_min\":round(mae_min,2), \"mae_log\":round(mae_log,4), \"rmse_log\":round(rmse_log,4)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c827dbe",
   "metadata": {},
   "source": [
    "## 5) Packaging — Save models & inference helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b779e059",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Save fitted models and provide simple inference helpers\n",
    "import joblib, os, numpy as np\n",
    "os.makedirs(\"/content/models\", exist_ok=True)\n",
    "joblib.dump(best_sev, \"/content/models/severity_best.pkl\")\n",
    "joblib.dump(best_dur, \"/content/models/duration_best.pkl\")\n",
    "print(\"Saved models in /content/models\")\n",
    "\n",
    "def preprocess_single_row(raw_row: dict, task=\"severity\"):\n",
    "    import pandas as pd, numpy as np\n",
    "    X = pd.DataFrame([raw_row])\n",
    "    Xt = preprocess_pipe.transform(X)\n",
    "    from numpy import where\n",
    "    if task==\"severity\":\n",
    "        Xt = Xt[:, np.where(mask_c)[0]]; Xt = Xt[:, top_c_idx][:, lean_c_idx]\n",
    "    else:\n",
    "        Xt = Xt[:, np.where(mask_r)[0]]; Xt = Xt[:, top_r_idx][:, lean_r_idx]\n",
    "    return Xt\n",
    "\n",
    "def predict_severity(raw_row: dict):\n",
    "    Xt = preprocess_single_row(raw_row, \"severity\"); return int(best_sev.predict(Xt)[0])\n",
    "\n",
    "def predict_duration_minutes(raw_row: dict):\n",
    "    Xt = preprocess_single_row(raw_row, \"duration\"); return float(np.expm1(best_dur.predict(Xt)[0]))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
